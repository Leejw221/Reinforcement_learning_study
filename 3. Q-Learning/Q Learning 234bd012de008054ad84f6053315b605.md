# Q Learning

ìƒì„±ì¼: 2025ë…„ 7ì›” 18ì¼ ì˜¤ì „ 9:32

**link:** https://huggingface.co/learn/deep-rl-course/unit2/introduction

<aside>
ğŸ’¡

RL agentì˜ ëª©í‘œ

- optimal policyì¸ $\pi^*$ë¥¼ ê°–ëŠ” ê²ƒì´ë‹¤.
</aside>

# 1. Optimal policyë¥¼ ì°¾ê¸° ìœ„í•œ ë‘ ê°€ì§€ ë°©ë²•

---

## 1. Value-based methodì˜ ê·¼ë³¸

- Value-based methodì—ì„œëŠ” value functionì„ í•™ìŠµí•œë‹¤.
    - value functionì€ stateë¥¼ í•´ë‹¹ stateì— ìˆëŠ” ì˜ˆìƒ ê°’ì— mappingí•˜ëŠ” ê²ƒì´ë‹¤.
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image.png)
    
- Stateì˜ ê°’ì€ agentê°€ í•´ë‹¹ stateì—ì„œ ì‹œì‘í•˜ì—¬ policyì— ë”°ë¼ í–‰ë™í•  ê²½ìš° ì–»ì„ ìˆ˜ ìˆëŠ” expectred discounted returnì´ë‹¤.
    - ê·¸ëŸ¬ë‚˜ ìš°ë¦¬ì˜ policyì— ë”°ë¼ í–‰ë™í•œë‹¤ëŠ” ê²ƒê³¼ ì—°ê²°í•˜ë©´ value-based methodì—ì„œëŠ” policyê°€ ì•„ë‹Œ value functionì„ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— policyê°€ ì—†ë‹¤.
    - ì´ ì˜ë¯¸ëŠ” value-based methodì—ì„œëŠ” ë¯¸ë¦¬ ì •ì˜ëœ policyì„ ì‚¬ìš©í•´ í–‰ë™ì„ ì„ íƒí•œë‹¤ëŠ” ê²ƒì´ë‹¤.
        - ë¯¸ë¦¬ ì •ì˜ëœ policyëŠ” ëŒ€í‘œì ìœ¼ë¡œ greedyê°€ ìˆë‹¤.
    - ë”°ë¼ì„œ value-based methodì—ì„œë„ policyì€ ì¡´ì¬í•œë‹¤.
        - ê°•í™”í•™ìŠµì—ì„œ policyëŠ” í•­ìƒ ì¡´ì¬í•œë‹¤.

## 2. Optimal policyë¥¼ ì°¾ê¸° ìœ„í•œ ë‘ ê°€ì§€ ë°©ë²•

1. Policy-based methods
    - Stateê°€ ì£¼ì–´ì§€ë©´ ìˆ˜í–‰í•  taskì„ ì„ íƒí•˜ë„ë¡ policyë¥¼ ì§ì ‘ í•™ìŠµí•œë‹¤.
        - ì´ ê²½ìš° value functionì´ ì—†ë‹¤.
    - PolicyëŠ” stateë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ í•´ë‹¹ stateì—ì„œ ì–´ë–¤ ì¡°ì¹˜ë¥¼ ì·¨í•´ì•¼ í•˜ëŠ”ì§€ ì¶œë ¥í•œë‹¤.
        - Deterministic policyëŠ” í–‰ë™ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ì¶œë ¥í•˜ëŠ” stochastic policyì™€ ë‹¬ë¦¬ stateê°€ ì£¼ì–´ì§„ í•˜ë‚˜ì˜ í–‰ë™ì„ ì¶œë ¥í•˜ëŠ” policyì´ë‹¤.
        - ë”°ë¼ì„œ policyë¥¼ ì§ì ‘ ì •ì˜í•˜ì§€ ì•Šê³ , trainingì„ ì •ì˜í•˜ëŠ” ê²ƒì´ë‹¤.
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%201.png)
    
2. Value-based methods
    - State ë˜ëŠ” state-action pairì˜ ê°’ì„ ì¶œë ¥í•˜ëŠ” value functionì„ í•™ìŠµí•˜ì—¬ ê°„ì ‘ì ìœ¼ë¡œ policyì„ ë„ì¶œí•œë‹¤.
        - Value functionì´ ì£¼ì–´ì§€ë©´ policyëŠ” í•˜ë‚˜ì˜ actionì„ ì·¨í•œë‹¤.
    - PolicyëŠ” trainingë˜ê±°ë‚˜ learningë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í•˜ë‚˜ì˜ í–‰ë™ì„ ì •ì˜í•´ì•¼ í•œë‹¤.
        - ì˜ˆë¥¼ ë“¤ì–´, value functionì´ ì£¼ì–´ì§€ë©´ í•­ìƒ ê°€ì¥ í° rewardë¡œ ì´ì–´ì§€ëŠ” actionì„ ì·¨í•˜ëŠ” policyë¥¼ ì›í•œë‹¤ë©´ â€˜Greedy Policyâ€™ì„ ë§Œë“œëŠ” ê²ƒì´ë‹¤.
    - Stateê°€ ì£¼ì–´ì§€ë©´ action-value functionì€ í•´ë‹¹ stateì—ì„œ ê° actionì˜ ê°’ì„ ì¶œë ¥í•œë‹¤.
        - ê·¸ëŸ° ë‹¤ìŒ ì‚¬ì „ ì •ì˜ëœ greedy policyëŠ” state ë˜ëŠ” action-state pairê°€ ì£¼ì–´ì§€ë©´ ê°€ì¥ ë†’ì€ ê°’ì„ ì‚°ì¶œí•˜ëŠ” actionì„ ì„ íƒí•œë‹¤.
3. ì •ë¦¬
    - ê²°ê³¼ì ìœ¼ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì–´ë–¤ ë°©ë²•ì„ ì‚¬ìš©í•˜ë“  policyëŠ” ìˆë‹¤.
    - Value-based methodsì˜ ê²½ìš° policyì„ trainingí•˜ì§€ ì•ŠëŠ”ë‹¤.
        - PolicyëŠ” value functionì—ì„œ ì œê³µí•˜ëŠ” ê°’ì„ ì‚¬ìš©í•˜ì—¬ taskë¥¼ ì„ íƒí•˜ëŠ” ê°„ë‹¨í•œ pre-specified functionì¼ ë¿ì´ë‹¤.
        - ëŒ€í‘œì ìœ¼ë¡œ Greedy policyê°€ ìˆë‹¤.
    - Policy-based trainingì—ì„œëŠ” policyì„ ì§ì ‘ í•™ìŠµí•˜ì—¬ ìµœì ì˜ policyì¸ $\pi^*$ì„ ì°¾ëŠ”ë‹¤.
    - Value-based trainingì—ì„œëŠ” ìµœì ì˜ value functionì„ ì°¾ëŠ” ê²ƒì´ ìµœì ì˜ policyë¥¼ ê°–ëŠ” ê²ƒìœ¼ë¡œ ì´ì–´ì§€ëŠ” ê²ƒì´ë‹¤.
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%202.png)
    

# 2. ë‘ ê°€ì§€ ìœ í˜•ì˜ Value-based function

1. State-value function
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%203.png)
    
    - ê° stateì— ëŒ€í•´ state-value functionì€ agentê°€ í•´ë‹¹ stateì—ì„œ ì‹œì‘í•œ ë‹¤ìŒ ê·¸ ì´í›„ì— policyì„ ì˜ì›íˆ ë”°ë¥¼ ê²½ìš° expected returnì„ ì¶œë ¥í•œë‹¤.
        - ì—¬ê¸°ì„œ â€˜policyì„ ì˜ì›íˆ ë”°ë¥¼ ê²½ìš°â€™ë¼ëŠ” ê²ƒì€ stateë¥¼ í†µí•´ policyë¼ëŠ” ê·œì¹™ìœ¼ë¡œ ê²°ì •ë˜ëŠ” actionì— ëŒ€í•˜ì—¬ actionì„ ê²°ì •í•˜ëŠ” ê·œì¹™ì„ ìœ ì§€í•œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.
    - ì´ ê²½ìš° ì´ stateê°€ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ ë¹„êµí•˜ê¸° ìœ„í•´ì„œ ëª¨ë“  stateì— ëŒ€í•œ valueë¥¼ ì•Œì•„ì•¼ í•˜ë¯€ë¡œ ëª¨ë“  stateì— ëŒ€í•œ valueë¥¼ ë¨¼ì € ê²°ì •ë˜ì–´ì•¼ í•œë‹¤.
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%204.png)
    
2. Action-value function
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%205.png)
    
    - Action-value functionì—ì„œ ê° state ë° action pairì— ëŒ€í•´ action-value functionì€ agentê°€ í•´ë‹¹ stateì—ì„œ ì‹œì‘í•˜ì—¬ í•´ë‹¹ taskì„ ìˆ˜í–‰í•œ ë‹¤ìŒ ê·¸ ì´í›„ì—ë„ ì˜ì›íˆ policyë¥¼ ë”°ë¥¼  ê²½ìš° expected returnì„ ì¶œë ¥í•˜ëŠ” ê²ƒì´ë‹¤.
        
        ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%206.png)
        
3. ì •ë¦¬
    - State-value functionì€ ìƒíƒœ $S_t$ì˜ ê°’ì„ ê³„ì‚°í•œë‹¤.
    - Action-value functionì€ state-action pair$(S_t,A_t)$ì˜ ê°’ì„ ê³„ì‚°í•˜ë¯€ë¡œ í•´ë‹¹ stateì—ì„œ í•´ë‹¹ actionì„ ìˆ˜í–‰í•˜ëŠ” ê°’ì„ ê³„ì‚°í•œë‹¤.
    - ë‘ ê²½ìš° ëª¨ë‘ ì–´ë–¤ value functionì„ ì„ íƒí•˜ë“  ë°˜í™˜ëœ ê°’ì€ expected returnì´ë‹¤.
    - ê·¸ëŸ¬ë‚˜ state ë˜ëŠ” state-action pairì˜ EACH ê°’ì„ ê³„ì‚°í•˜ë ¤ë©´ agentê°€ í•´ë‹¹ stateì—ì„œ ì‹œì‘í–ˆì„ ë•Œ ì–»ì„ ìˆ˜ ìˆëŠ” ëª¨ë“  ë³´ìƒì„ í•©ì‚°í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤.
    - ì´ë¥¼ ìœ„í•´ Bellman equationì„ ì´ìš©í•œë‹¤.
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%207.png)
    

# 3. Bellman Equation

---

1. Bellman equationì€ state value ë˜ëŠ” state-action valueë¥¼ ë‹¨ìˆœí™”í•  ìˆ˜ ìˆë‹¤.
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%208.png)
    
2. State valueì¸ $V(S_t)$í•˜ë©´ í•´ë‹¹ stateì—ì„œ ì‹œì‘í•˜ì—¬ episodeê°€ ëë‚  ë•Œê¹Œì§€ policy $\pi$ë¥¼ ì¼ê´€ë˜ê²Œ ë”°ë¼ í–‰ë™í–ˆì„ ëŒ€ì˜ expectred discounted returnì´ë‹¤.
3. ë‹¤ìŒê³¼ ê°™ì€ Greedy Policyë¡œ ì˜ˆì‹œë¥¼ ë“¤ ìˆ˜ ìˆë‹¤. ê³„ì‚°ì„ ë‹¨ìˆœí™”í•˜ê¸° ìœ„í•´ discountëŠ” ì ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.
    - $V(S_t)$ë¥¼ ê³„ì‚°í•˜ë ¤ë©´ expected rewardì˜ í•©ì„ ê³„ì‚°í•´ì•¼ í•œë‹¤.
    - State 1ì˜ ê°’ì„ ê³„ì‚°í•˜ë ¤ë©´ agentê°€ í•´ë‹¹ stateì—ì„œ ì‹œì‘í•œ ë‹¤ìŒ ëª¨ë“  time stepì— ëŒ€í•´ greedy policyì„ ë”°ëì„ ë•Œì˜ rewardì˜ í•©ê³„ì´ë‹¤.
        - ì—¬ê¸°ì„œ greedy policyë¼ëŠ” ê²ƒì€ ìµœìƒì˜ state valueë¡œ ì´ì–´ì§€ë„ë¡ actionì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë‹¤.
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%209.png)
    
4. ê·¸ëŸ° ë‹¤ìŒ $V(S_{t+1})$ì„ ê³„ì‚°í•˜ë ¤ë©´ $S_{t+1}$ stateì—ì„œ ì‹œì‘í•˜ëŠ” returnì„ ê³„ì‚°í•´ì•¼í•œë‹¤.
    - State 2ì˜ ê°’ì„ ê³„ì‚°í•˜ë ¤ë©´ agentê°€ í•´ë‹¹ stateì—ì„œ ì‹œì‘í•œ ë‹¤ìŒ ëª¨ë“  time stepì— ëŒ€í•œ policyì„ ë”°ëì„ ê²½ìš° returnì˜ í•©ê³„ë¥¼ ê³„ì‚°í•´ì•¼ í•œë‹¤.
        
        ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%2010.png)
        
5. ì´ë ‡ë“¯ ë‹¤ë¥¸ state valueë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•´ì•¼ í•œë‹¤.
    - ê° state ë˜ëŠ” ê° state-action pairì— ëŒ€í•œ expected returnì„ ê³„ì‚°í•˜ëŠ” ëŒ€ì‹  Bellman equationì„ ì´ìš©í•˜ì—¬ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.
6. Bellman equationì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ë™í•˜ëŠ” ì¬ê·€ ë°©ì •ì‹(recursive equation)ì´ë‹¤.
    - ì²˜ìŒë¶€í„° ê° stateì— ëŒ€í•´ ì‹œì‘í•˜ì—¬ returnì„ ê³„ì‚°í•˜ëŠ” ëŒ€ì‹  ëª¨ë“  stateì˜ ê°’ì„ ë‹¤ìŒê³¼ ê°™ì´ ê°„ì£¼í•  ìˆ˜ ìˆë‹¤.
        - $R_{t+1}+(\gamma*V(S_{t+1}))$
            - $R_{t+1}:$ The immediate reward
            - $(\gamma*V(S_{t+1})):$ The discounted value of the state that follows
        
        ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%2011.png)
        
7. ìœ„ ì˜ˆì œì—ì„œ ì„¤ëª…í•œ ê²ƒì„ ë‹¤ì‹œ ì„¤ëª…í•˜ë©´, state 1ì˜ valueëŠ” í•´ë‹¹ stateì—ì„œ ì‹œì‘í•  ê²½ìš° expected cumulative returnê³¼ ê°™ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.
    - State 1ì˜ ê°’ì€ agentê°€ í•´ë‹¹ state 1ì—ì„œ ì‹œì‘í•œ ë‹¤ìŒ ëª¨ë“  time stepì— ëŒ€í•œ policyì„ ë”°ëì„ ê²½ìš° returnì˜ í•©ê³„ì´ë‹¤.
        - $V(S_t)=R_{t+1}+\gamma*V(S_{t+1})$
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%2012.png)
    
8. ê³„ì‚°ì„ ë‹¨ìˆœí™”í•˜ê¸° ìœ„í•´ì„œ discountëŠ” 1ë¡œ ì§€ì •í•œë‹¤.
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%2013.png)
    
9. ì •ë¦¬
    - Bellman equationì€ ê° valueì„ ê¸´ ê³¼ì •ì¸ expected  returnì˜ í•©ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ëŒ€ì‹  immediate rewardì™€ the discounted value of the state that followsì˜ í•©ìœ¼ë¡œ ê³„ì‚°í•œë‹¤.