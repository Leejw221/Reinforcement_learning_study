# Q Learning

생성일: 2025년 7월 18일 오전 9:32

**link:** https://huggingface.co/learn/deep-rl-course/unit2/introduction

<aside>
💡

RL agent의 목표

- optimal policy인 $\pi^*$를 갖는 것이다.
</aside>

# 1. Optimal policy를 찾기 위한 두 가지 방법

---

## 1. Value-based method의 근본

- Value-based method에서는 value function을 학습한다.
    - value function은 state를 해당 state에 있는 예상 값에 mapping하는 것이다.
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image.png)
    
- State의 값은 agent가 해당 state에서 시작하여 policy에 따라 행동할 경우 얻을 수 있는 expectred discounted return이다.
    - 그러나 우리의 policy에 따라 행동한다는 것과 연결하면 value-based method에서는 policy가 아닌 value function을 학습하기 때문에 policy가 없다.
    - 이 의미는 value-based method에서는 미리 정의된 policy을 사용해 행동을 선택한다는 것이다.
        - 미리 정의된 policy는 대표적으로 greedy가 있다.
    - 따라서 value-based method에서도 policy은 존재한다.
        - 강화학습에서 policy는 항상 존재한다.

## 2. Optimal policy를 찾기 위한 두 가지 방법

1. Policy-based methods
    - State가 주어지면 수행할 task을 선택하도록 policy를 직접 학습한다.
        - 이 경우 value function이 없다.
    - Policy는 state를 입력으로 받아 해당 state에서 어떤 조치를 취해야 하는지 출력한다.
        - Deterministic policy는 행동에 대한 확률 분포를 출력하는 stochastic policy와 달리 state가 주어진 하나의 행동을 출력하는 policy이다.
        - 따라서 policy를 직접 정의하지 않고, training을 정의하는 것이다.
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%201.png)
    
2. Value-based methods
    - State 또는 state-action pair의 값을 출력하는 value function을 학습하여 간접적으로 policy을 도출한다.
        - Value function이 주어지면 policy는 하나의 action을 취한다.
    - Policy는 training되거나 learning되는 것이 아니라 하나의 행동을 정의해야 한다.
        - 예를 들어, value function이 주어지면 항상 가장 큰 reward로 이어지는 action을 취하는 policy를 원한다면 ‘Greedy Policy’을 만드는 것이다.
    - State가 주어지면 action-value function은 해당 state에서 각 action의 값을 출력한다.
        - 그런 다음 사전 정의된 greedy policy는 state 또는 action-state pair가 주어지면 가장 높은 값을 산출하는 action을 선택한다.
3. 정리
    - 결과적으로 문제를 해결하기 위해 어떤 방법을 사용하든 policy는 있다.
    - Value-based methods의 경우 policy을 training하지 않는다.
        - Policy는 value function에서 제공하는 값을 사용하여 task를 선택하는 간단한 pre-specified function일 뿐이다.
        - 대표적으로 Greedy policy가 있다.
    - Policy-based training에서는 policy을 직접 학습하여 최적의 policy인 $\pi^*$을 찾는다.
    - Value-based training에서는 최적의 value function을 찾는 것이 최적의 policy를 갖는 것으로 이어지는 것이다.
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%202.png)
    

# 2. 두 가지 유형의 Value-based function

1. State-value function
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%203.png)
    
    - 각 state에 대해 state-value function은 agent가 해당 state에서 시작한 다음 그 이후에 policy을 영원히 따를 경우 expected return을 출력한다.
        - 여기서 ‘policy을 영원히 따를 경우’라는 것은 state를 통해 policy라는 규칙으로 결정되는 action에 대하여 action을 결정하는 규칙을 유지한다는 의미이다.
    - 이 경우 이 state가 얼마나 좋은지 비교하기 위해서 모든 state에 대한 value를 알아야 하므로 모든 state에 대한 value를 먼저 결정되어야 한다.
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%204.png)
    
2. Action-value function
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%205.png)
    
    - Action-value function에서 각 state 및 action pair에 대해 action-value function은 agent가 해당 state에서 시작하여 해당 task을 수행한 다음 그 이후에도 영원히 policy를 따를  경우 expected return을 출력하는 것이다.
        
        ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%206.png)
        
3. 정리
    - State-value function은 상태 $S_t$의 값을 계산한다.
    - Action-value function은 state-action pair$(S_t,A_t)$의 값을 계산하므로 해당 state에서 해당 action을 수행하는 값을 계산한다.
    - 두 경우 모두 어떤 value function을 선택하든 반환된 값은 expected return이다.
    - 그러나 state 또는 state-action pair의 EACH 값을 계산하려면 agent가 해당 state에서 시작했을 때 얻을 수 있는 모든 보상을 합산해야 한다는 것이다.
    - 이를 위해 Bellman equation을 이용한다.
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%207.png)
    

# 3. Bellman Equation

---

1. Bellman equation은 state value 또는 state-action value를 단순화할 수 있다.
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%208.png)
    
2. State value인 $V(S_t)$하면 해당 state에서 시작하여 episode가 끝날 때까지 policy $\pi$를 일관되게 따라 행동했을 대의 expectred discounted return이다.
3. 다음과 같은 Greedy Policy로 예시를 들 수 있다. 계산을 단순화하기 위해 discount는 적용하지 않는다.
    - $V(S_t)$를 계산하려면 expected reward의 합을 계산해야 한다.
    - State 1의 값을 계산하려면 agent가 해당 state에서 시작한 다음 모든 time step에 대해 greedy policy을 따랐을 때의 reward의 합계이다.
        - 여기서 greedy policy라는 것은 최상의 state value로 이어지도록 action을 수행하는 것이다.
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%209.png)
    
4. 그런 다음 $V(S_{t+1})$을 계산하려면 $S_{t+1}$ state에서 시작하는 return을 계산해야한다.
    - State 2의 값을 계산하려면 agent가 해당 state에서 시작한 다음 모든 time step에 대한 policy을 따랐을 경우 return의 합계를 계산해야 한다.
        
        ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%2010.png)
        
5. 이렇듯 다른 state value를 계산하는 과정을 반복해야 한다.
    - 각 state 또는 각 state-action pair에 대한 expected return을 계산하는 대신 Bellman equation을 이용하여 계산할 수 있다.
6. Bellman equation은 다음과 같이 작동하는 재귀 방정식(recursive equation)이다.
    - 처음부터 각 state에 대해 시작하여 return을 계산하는 대신 모든 state의 값을 다음과 같이 간주할 수 있다.
        - $R_{t+1}+(\gamma*V(S_{t+1}))$
            - $R_{t+1}:$ The immediate reward
            - $(\gamma*V(S_{t+1})):$ The discounted value of the state that follows
        
        ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%2011.png)
        
7. 위 예제에서 설명한 것을 다시 설명하면, state 1의 value는 해당 state에서 시작할 경우 expected cumulative return과 같다고 할 수 있다.
    - State 1의 값은 agent가 해당 state 1에서 시작한 다음 모든 time step에 대한 policy을 따랐을 경우 return의 합계이다.
        - $V(S_t)=R_{t+1}+\gamma*V(S_{t+1})$
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%2012.png)
    
8. 계산을 단순화하기 위해서 discount는 1로 지정한다.
    
    ![image.png](Q%20Learning%20234bd012de008054ad84f6053315b605/image%2013.png)
    
9. 정리
    - Bellman equation은 각 value을 긴 과정인 expected  return의 합으로 계산하는 대신 immediate reward와 the discounted value of the state that follows의 합으로 계산한다.